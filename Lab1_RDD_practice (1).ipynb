{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8b3ba45-9b0b-43f2-9c84-2ce8e7aec6ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For help, look here:\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0145c6ca-d9f7-4bd7-8309-3fee0a69c3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/COVID/</td><td>COVID/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td><td>1532468253000</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td><td>1455043490000</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/delta-sharing/</td><td>delta-sharing/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flower_photos/</td><td>flower_photos/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flowers/</td><td>flowers/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/identifying-campaign-effectiveness/</td><td>identifying-campaign-effectiveness/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/med-images/</td><td>med-images/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/media/</td><td>media/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi/</td><td>nyctaxi/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi-with-zipcodes/</td><td>nyctaxi-with-zipcodes/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/retail-org/</td><td>retail-org/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/rwe/</td><td>rwe/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/timeseries/</td><td>timeseries/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/travel_recommendations_realtime/</td><td>travel_recommendations_realtime/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/warmup/</td><td>warmup/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wine-quality/</td><td>wine-quality/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/COVID/",
         "COVID/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/README.md",
         "README.md",
         976,
         1532468253000
        ],
        [
         "dbfs:/databricks-datasets/Rdatasets/",
         "Rdatasets/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/SPARK_README.md",
         "SPARK_README.md",
         3359,
         1455043490000
        ],
        [
         "dbfs:/databricks-datasets/adult/",
         "adult/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/airlines/",
         "airlines/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/amazon/",
         "amazon/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/asa/",
         "asa/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/atlas_higgs/",
         "atlas_higgs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/bikeSharing/",
         "bikeSharing/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cctvVideos/",
         "cctvVideos/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/credit-card-fraud/",
         "credit-card-fraud/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs100/",
         "cs100/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs110x/",
         "cs110x/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs190/",
         "cs190/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/data.gov/",
         "data.gov/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/definitive-guide/",
         "definitive-guide/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/delta-sharing/",
         "delta-sharing/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flights/",
         "flights/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flower_photos/",
         "flower_photos/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flowers/",
         "flowers/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/genomics/",
         "genomics/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/hail/",
         "hail/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/identifying-campaign-effectiveness/",
         "identifying-campaign-effectiveness/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/iot/",
         "iot/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/iot-stream/",
         "iot-stream/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/learning-spark/",
         "learning-spark/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/learning-spark-v2/",
         "learning-spark-v2/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/lending-club-loan-stats/",
         "lending-club-loan-stats/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/med-images/",
         "med-images/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/media/",
         "media/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/mnist-digits/",
         "mnist-digits/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/news20.binary/",
         "news20.binary/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi/",
         "nyctaxi/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi-with-zipcodes/",
         "nyctaxi-with-zipcodes/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/online_retail/",
         "online_retail/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/overlap-join/",
         "overlap-join/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/power-plant/",
         "power-plant/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/retail-org/",
         "retail-org/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/rwe/",
         "rwe/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sai-summit-2019-sf/",
         "sai-summit-2019-sf/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sample_logs/",
         "sample_logs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/samples/",
         "samples/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sfo_customer_survey/",
         "sfo_customer_survey/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sms_spam_collection/",
         "sms_spam_collection/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/songs/",
         "songs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/",
         "structured-streaming/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/timeseries/",
         "timeseries/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/tpch/",
         "tpch/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/travel_recommendations_realtime/",
         "travel_recommendations_realtime/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/warmup/",
         "warmup/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/weather/",
         "weather/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wiki/",
         "wiki/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wikipedia-datasets/",
         "wikipedia-datasets/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wine-quality/",
         "wine-quality/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check out the pre-loaded dataset\n",
    "display(dbutils.fs.ls('dbfs:/databricks-datasets/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "195d300a-63f0-4dc8-b0c4-bc1c4cf6a9b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51aa0658-e68f-4a62-8962-5b1baa0ede76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a rdd (sc = SparkContext)\n",
    "rdd = sc.textFile(\"dbfs:/databricks-datasets/SPARK_README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaf066b0-5738-4d2d-a225-8ae0d44d3a78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: ['# Apache Spark',\n '',\n 'Spark is a fast and general cluster computing system for Big Data. It provides',\n 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n 'supports general computation graphs for data analysis. It also supports a',\n 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n 'MLlib for machine learning, GraphX for graph processing,',\n 'and Spark Streaming for stream processing.',\n '',\n '<http://spark.apache.org/>',\n '',\n '',\n '## Online Documentation',\n '',\n 'You can find the latest Spark documentation, including a programming',\n 'guide, on the [project web page](http://spark.apache.org/documentation.html)',\n 'and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).',\n 'This README file only contains basic setup instructions.',\n '',\n '## Building Spark']"
     ]
    }
   ],
   "source": [
    "# Read 20 lines \n",
    "rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c90957-5053-4d75-8930-9facef942a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\nApache\nSpark\n\nSpark\nis\na\nfast\nand\ngeneral\ncluster\ncomputing\nsystem\nfor\nBig\nData.\nIt\nprovides\nhigh-level\nAPIs\nin\nScala,\nJava,\nPython,\nand\nR,\nand\nan\noptimized\nengine\nthat\nsupports\ngeneral\ncomputation\ngraphs\nfor\ndata\nanalysis.\nIt\nalso\nsupports\na\nrich\nset\nof\nhigher-level\ntools\nincluding\nSpark\nSQL\nfor\nSQL\nand\nDataFrames,\nMLlib\nfor\nmachine\nlearning,\nGraphX\nfor\ngraph\nprocessing,\nand\nSpark\nStreaming\nfor\nstream\nprocessing.\n\n<http://spark.apache.org/>\n\n\n##\nOnline\nDocumentation\n\nYou\ncan\nfind\nthe\nlatest\nSpark\ndocumentation,\nincluding\na\nprogramming\nguide,\non\nthe\n[project\nweb\npage](http://spark.apache.org/documentation.html)\nand\n[project\nwiki](https://cwiki.apache.org/confluence/display/SPARK).\nThis\nREADME\nfile\nonly\ncontains\nbasic\nsetup\ninstructions.\n\n##\nBuilding\nSpark\n\nSpark\nis\nbuilt\nusing\n[Apache\nMaven](http://maven.apache.org/).\nTo\nbuild\nSpark\nand\nits\nexample\nprograms,\nrun:\n\n\n\n\n\nbuild/mvn\n-DskipTests\nclean\npackage\n\n(You\ndo\nnot\nneed\nto\ndo\nthis\nif\nyou\ndownloaded\na\npre-built\npackage.)\nMore\ndetailed\ndocumentation\nis\navailable\nfrom\nthe\nproject\nsite,\nat\n[\"Building\nSpark\"](http://spark.apache.org/docs/latest/building-spark.html).\n\n##\nInteractive\nScala\nShell\n\nThe\neasiest\nway\nto\nstart\nusing\nSpark\nis\nthrough\nthe\nScala\nshell:\n\n\n\n\n\n./bin/spark-shell\n\nTry\nthe\nfollowing\ncommand,\nwhich\nshould\nreturn\n1000:\n\n\n\n\n\nscala>\nsc.parallelize(1\nto\n1000).count()\n\n##\nInteractive\nPython\nShell\n\nAlternatively,\nif\nyou\nprefer\nPython,\nyou\ncan\nuse\nthe\nPython\nshell:\n\n\n\n\n\n./bin/pyspark\n\nAnd\nrun\nthe\nfollowing\ncommand,\nwhich\nshould\nalso\nreturn\n1000:\n\n\n\n\n\n>>>\nsc.parallelize(range(1000)).count()\n\n##\nExample\nPrograms\n\nSpark\nalso\ncomes\nwith\nseveral\nsample\nprograms\nin\nthe\n`examples`\ndirectory.\nTo\nrun\none\nof\nthem,\nuse\n`./bin/run-example\n<class>\n[params]`.\nFor\nexample:\n\n\n\n\n\n./bin/run-example\nSparkPi\n\nwill\nrun\nthe\nPi\nexample\nlocally.\n\nYou\ncan\nset\nthe\nMASTER\nenvironment\nvariable\nwhen\nrunning\nexamples\nto\nsubmit\nexamples\nto\na\ncluster.\nThis\ncan\nbe\na\nmesos://\nor\nspark://\nURL,\n\"yarn\"\nto\nrun\non\nYARN,\nand\n\"local\"\nto\nrun\nlocally\nwith\none\nthread,\nor\n\"local[N]\"\nto\nrun\nlocally\nwith\nN\nthreads.\nYou\ncan\nalso\nuse\nan\nabbreviated\nclass\nname\nif\nthe\nclass\nis\nin\nthe\n`examples`\npackage.\nFor\ninstance:\n\n\n\n\n\nMASTER=spark://host:7077\n./bin/run-example\nSparkPi\n\nMany\nof\nthe\nexample\nprograms\nprint\nusage\nhelp\nif\nno\nparams\nare\ngiven.\n\n##\nRunning\nTests\n\nTesting\nfirst\nrequires\n[building\nSpark](#building-spark).\nOnce\nSpark\nis\nbuilt,\ntests\ncan\nbe\nrun\nusing:\n\n\n\n\n\n./dev/run-tests\n\nPlease\nsee\nthe\nguidance\non\nhow\nto\n[run\ntests\nfor\na\nmodule,\nor\nindividual\ntests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).\n\n##\nA\nNote\nAbout\nHadoop\nVersions\n\nSpark\nuses\nthe\nHadoop\ncore\nlibrary\nto\ntalk\nto\nHDFS\nand\nother\nHadoop-supported\nstorage\nsystems.\nBecause\nthe\nprotocols\nhave\nchanged\nin\ndifferent\nversions\nof\nHadoop,\nyou\nmust\nbuild\nSpark\nagainst\nthe\nsame\nversion\nthat\nyour\ncluster\nruns.\n\nPlease\nrefer\nto\nthe\nbuild\ndocumentation\nat\n[\"Specifying\nthe\nHadoop\nVersion\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\nfor\ndetailed\nguidance\non\nbuilding\nfor\na\nparticular\ndistribution\nof\nHadoop,\nincluding\nbuilding\nfor\nparticular\nHive\nand\nHive\nThriftserver\ndistributions.\n\n##\nConfiguration\n\nPlease\nrefer\nto\nthe\n[Configuration\nGuide](http://spark.apache.org/docs/latest/configuration.html)\nin\nthe\nonline\ndocumentation\nfor\nan\noverview\non\nhow\nto\nconfigure\nSpark.\n"
     ]
    }
   ],
   "source": [
    "# Example: lambda functions  \n",
    "words = rdd.flatMap(lambda lines: lines.split(\" \"))\n",
    "\n",
    "for w in words.collect():\n",
    "  print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222ddcdd-271e-4703-9cd9-e2ed3bdf92aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#: 1\nApache: 1\nSpark: 13\nis: 6\na: 8\nfast: 1\nand: 10\ngeneral: 2\ncluster: 2\ncomputing: 1\nsystem: 1\nfor: 11\nBig: 1\nData.: 1\nIt: 2\nprovides: 1\nhigh-level: 1\nAPIs: 1\nin: 5\nScala,: 1\n"
     ]
    }
   ],
   "source": [
    "# Take the previous function and\n",
    "# 1. count the occurence of each word\n",
    "\n",
    "# Load text file into DataFrame\n",
    "df = spark.read.text(\"dbfs:/databricks-datasets/SPARK_README.md\")\n",
    "\n",
    "# Count word occurrences\n",
    "word_counts = (\n",
    "    rdd.flatMap(lambda line: line.split(\" \"))  # Split each line into words\n",
    "    .map(lambda word: word.strip())  # Remove extra spaces\n",
    "    .filter(lambda word: word != \"\")  # Remove empty words\n",
    "    .countByValue()  # Count occurrences of each word\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for word, count in list(word_counts.items())[:20]:  # Show first 20 results\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91d9a83d-2ecd-4f59-80b6-dae5646d91c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# apache spark\n\nspark is a fast and general cluster computing system for big data. it provides\nhigh-level apis in scala, java, python, and r, and an optimized engine that\nsupports general computation graphs for data analysis. it also supports a\nrich set of higher-level tools including spark sql for sql and dataframes,\nmllib for machine learning, graphx for graph processing,\nand spark streaming for stream processing.\n\n<http://spark.apache.org/>\n\n\n## online documentation\n\nyou can find the latest spark documentation, including a programming\nguide, on the [project web page](http://spark.apache.org/documentation.html)\nand [project wiki](https://cwiki.apache.org/confluence/display/spark).\nthis readme file only contains basic setup instructions.\n\n## building spark\n\nspark is built using [apache maven](http://maven.apache.org/).\nto build spark and its example programs, run:\n\n    build/mvn -dskiptests clean package\n\n(you do not need to do this if you downloaded a pre-built package.)\nmore detailed documentation is available from the project site, at\n[\"building spark\"](http://spark.apache.org/docs/latest/building-spark.html).\n\n## interactive scala shell\n\nthe easiest way to start using spark is through the scala shell:\n\n    ./bin/spark-shell\n\ntry the following command, which should return 1000:\n\n    scala> sc.parallelize(1 to 1000).count()\n\n## interactive python shell\n\nalternatively, if you prefer python, you can use the python shell:\n\n    ./bin/pyspark\n\nand run the following command, which should also return 1000:\n\n    >>> sc.parallelize(range(1000)).count()\n\n## example programs\n\nspark also comes with several sample programs in the `examples` directory.\nto run one of them, use `./bin/run-example <class> [params]`. for example:\n\n    ./bin/run-example sparkpi\n\nwill run the pi example locally.\n\nyou can set the master environment variable when running examples to submit\nexamples to a cluster. this can be a mesos:// or spark:// url,\n\"yarn\" to run on yarn, and \"local\" to run\nlocally with one thread, or \"local[n]\" to run locally with n threads. you\ncan also use an abbreviated class name if the class is in the `examples`\npackage. for instance:\n\n    master=spark://host:7077 ./bin/run-example sparkpi\n\nmany of the example programs print usage help if no params are given.\n\n## running tests\n\ntesting first requires [building spark](#building-spark). once spark is built, tests\ncan be run using:\n\n    ./dev/run-tests\n\nplease see the guidance on how to\n[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools).\n\n## a note about hadoop versions\n\nspark uses the hadoop core library to talk to hdfs and other hadoop-supported\nstorage systems. because the protocols have changed in different versions of\nhadoop, you must build spark against the same version that your cluster runs.\n\nplease refer to the build documentation at\n[\"specifying the hadoop version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\nfor detailed guidance on building for a particular distribution of hadoop, including\nbuilding for particular hive and hive thriftserver distributions.\n\n## configuration\n\nplease refer to the [configuration guide](http://spark.apache.org/docs/latest/configuration.html)\nin the online documentation for an overview on how to configure spark.\nOut[9]: ['# Apache Spark',\n '',\n 'Spark is a fast and general cluster computing system for Big Data. It provides',\n 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n 'supports general computation graphs for data analysis. It also supports a',\n 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n 'MLlib for machine learning, GraphX for graph processing,',\n 'and Spark Streaming for stream processing.',\n '',\n '<http://spark.apache.org/>',\n '',\n '',\n '## Online Documentation',\n '',\n 'You can find the latest Spark documentation, including a programming',\n 'guide, on the [project web page](http://spark.apache.org/documentation.html)',\n 'and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).',\n 'This README file only contains basic setup instructions.',\n '',\n '## Building Spark']"
     ]
    }
   ],
   "source": [
    "# 2. change all capital letters to lower case\n",
    "lower_rdd = rdd.map(lambda line: line.lower())\n",
    "for line in lower_rdd.collect():\n",
    "  print(line)\n",
    "\n",
    "# Show results\n",
    "rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a41d715-efd1-48be-8c6d-f822e71fb4bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\napache\nspark\nspark\nis\na\nfast\ngeneral\ncluster\ncomputing\nsystem\nfor\nbig\ndata.\nit\nprovides\nhigh-level\napis\nscala,\njava,\npython,\nr,\noptimized\nengine\nthat\nsupports\ngeneral\ncomputation\ngraphs\nfor\ndata\nanalysis.\nit\nalso\nsupports\na\nrich\nset\nof\nhigher-level\ntools\nincluding\nspark\nsql\nfor\nsql\ndataframes,\nmllib\nfor\nmachine\nlearning,\ngraphx\nfor\ngraph\nprocessing,\nspark\nstreaming\nfor\nstream\nprocessing.\n<http://spark.apache.org/>\n##\nonline\ndocumentation\nyou\ncan\nfind\nlatest\nspark\ndocumentation,\nincluding\na\nprogramming\nguide,\non\n[project\nweb\npage](http://spark.apache.org/documentation.html)\n[project\nwiki](https://cwiki.apache.org/confluence/display/spark).\nthis\nreadme\nfile\nonly\ncontains\nbasic\nsetup\ninstructions.\n##\nbuilding\nspark\nspark\nis\nbuilt\nusing\n[apache\nmaven](http://maven.apache.org/).\nbuild\nspark\nits\nexample\nprograms,\nrun:\nbuild/mvn\n-dskiptests\nclean\npackage\n(you\ndo\nnot\nneed\ndo\nthis\nif\nyou\ndownloaded\na\npre-built\npackage.)\nmore\ndetailed\ndocumentation\nis\navailable\nfrom\nproject\nsite,\n[\"building\nspark\"](http://spark.apache.org/docs/latest/building-spark.html).\n##\ninteractive\nscala\nshell\neasiest\nway\nstart\nusing\nspark\nis\nthrough\nscala\nshell:\n./bin/spark-shell\ntry\nfollowing\ncommand,\nwhich\nshould\nreturn\n1000:\nscala>\nsc.parallelize(1\n1000).count()\n##\ninteractive\npython\nshell\nalternatively,\nif\nyou\nprefer\npython,\nyou\ncan\nuse\npython\nshell:\n./bin/pyspark\nrun\nfollowing\ncommand,\nwhich\nshould\nalso\nreturn\n1000:\n>>>\nsc.parallelize(range(1000)).count()\n##\nexample\nprograms\nspark\nalso\ncomes\nwith\nseveral\nsample\nprograms\n`examples`\ndirectory.\nrun\none\nof\nthem,\nuse\n`./bin/run-example\n<class>\n[params]`.\nfor\nexample:\n./bin/run-example\nsparkpi\nwill\nrun\npi\nexample\nlocally.\nyou\ncan\nset\nmaster\nenvironment\nvariable\nwhen\nrunning\nexamples\nsubmit\nexamples\na\ncluster.\nthis\ncan\nbe\na\nmesos://\nor\nspark://\nurl,\n\"yarn\"\nrun\non\nyarn,\n\"local\"\nrun\nlocally\nwith\none\nthread,\nor\n\"local[n]\"\nrun\nlocally\nwith\nn\nthreads.\nyou\ncan\nalso\nuse\nabbreviated\nclass\nname\nif\nclass\nis\n`examples`\npackage.\nfor\ninstance:\nmaster=spark://host:7077\n./bin/run-example\nsparkpi\nmany\nof\nexample\nprograms\nprint\nusage\nhelp\nif\nno\nparams\nare\ngiven.\n##\nrunning\ntests\ntesting\nfirst\nrequires\n[building\nspark](#building-spark).\nonce\nspark\nis\nbuilt,\ntests\ncan\nbe\nrun\nusing:\n./dev/run-tests\nplease\nsee\nguidance\non\nhow\n[run\ntests\nfor\na\nmodule,\nor\nindividual\ntests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools).\n##\na\nnote\nabout\nhadoop\nversions\nspark\nuses\nhadoop\ncore\nlibrary\ntalk\nhdfs\nother\nhadoop-supported\nstorage\nsystems.\nbecause\nprotocols\nhave\nchanged\ndifferent\nversions\nof\nhadoop,\nyou\nmust\nbuild\nspark\nagainst\nsame\nversion\nthat\nyour\ncluster\nruns.\nplease\nrefer\nbuild\ndocumentation\n[\"specifying\nhadoop\nversion\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\nfor\ndetailed\nguidance\non\nbuilding\nfor\na\nparticular\ndistribution\nof\nhadoop,\nincluding\nbuilding\nfor\nparticular\nhive\nhive\nthriftserver\ndistributions.\n##\nconfiguration\nplease\nrefer\n[configuration\nguide](http://spark.apache.org/docs/latest/configuration.html)\nonline\ndocumentation\nfor\noverview\non\nhow\nconfigure\nspark.\n"
     ]
    }
   ],
   "source": [
    "# 3. eliminate stopwords \n",
    "stop_words = ['and', 'to', 'in', 'at', 'the', 'an']\n",
    "filtered_rdd = (\n",
    "    rdd.flatMap(lambda line: line.split())    # Split lines into words\n",
    "        .map(lambda word: word.lower())       # Convert each word to lowercase\n",
    "        .filter(lambda word: word not in stop_words)  # Remove stopwords\n",
    ")\n",
    "\n",
    "# Collect and print the filtered content\n",
    "for word in filtered_rdd.collect():\n",
    "  print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b3b435-df3e-4181-8eb4-453cc354e576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: ['',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '\"local\"',\n '\"local[N]\"',\n '\"yarn\"',\n '#',\n '##',\n '##',\n '##',\n '##',\n '##',\n '##',\n '##',\n '##',\n '(You',\n '-DskipTests',\n './bin/pyspark',\n './bin/run-example',\n './bin/run-example',\n './bin/spark-shell',\n './dev/run-tests',\n '1000).count()',\n '1000:',\n '1000:',\n '<class>',\n '<http://spark.apache.org/>',\n '>>>',\n '[\"Building',\n '[\"Specifying',\n '[Apache',\n '[building',\n '[Configuration',\n '[params]`.',\n '[project',\n '[project',\n '[run',\n '`./bin/run-example',\n '`examples`',\n '`examples`',\n 'a',\n 'a',\n 'a',\n 'a',\n 'a',\n 'a',\n 'a',\n 'A',\n 'a',\n 'abbreviated',\n 'About',\n 'against',\n 'also',\n 'also',\n 'also',\n 'also',\n 'Alternatively,',\n 'an',\n 'an',\n 'an',\n 'analysis.',\n 'and',\n 'and',\n 'and',\n 'and',\n 'and',\n 'and',\n 'and',\n 'And',\n 'and',\n 'and',\n 'and',\n 'Apache',\n 'APIs',\n 'are',\n 'at',\n 'at',\n 'available',\n 'basic',\n 'be',\n 'be',\n 'Because',\n 'Big',\n 'build',\n 'build',\n 'build',\n 'build/mvn',\n 'Building',\n 'building',\n 'building',\n 'built',\n 'built,',\n 'can',\n 'can',\n 'can',\n 'can',\n 'can',\n 'can',\n 'changed',\n 'class',\n 'class',\n 'clean',\n 'cluster',\n 'cluster',\n 'cluster.',\n 'comes',\n 'command,',\n 'command,',\n 'computation',\n 'computing',\n 'Configuration',\n 'configure',\n 'contains',\n 'core',\n 'data',\n 'Data.',\n 'DataFrames,',\n 'detailed',\n 'detailed',\n 'different',\n 'directory.',\n 'distribution',\n 'distributions.',\n 'do',\n 'do',\n 'Documentation',\n 'documentation',\n 'documentation',\n 'documentation',\n 'documentation,',\n 'downloaded',\n 'easiest',\n 'engine',\n 'environment',\n 'example',\n 'Example',\n 'example',\n 'example',\n 'example:',\n 'examples',\n 'examples',\n 'fast',\n 'file',\n 'find',\n 'first',\n 'following',\n 'following',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'For',\n 'For',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'from',\n 'general',\n 'general',\n 'given.',\n 'graph',\n 'graphs',\n 'GraphX',\n 'guidance',\n 'guidance',\n 'guide,',\n 'Guide](http://spark.apache.org/docs/latest/configuration.html)',\n 'Hadoop',\n 'Hadoop',\n 'Hadoop',\n 'Hadoop,',\n 'Hadoop,',\n 'Hadoop-supported',\n 'have',\n 'HDFS',\n 'help',\n 'high-level',\n 'higher-level',\n 'Hive',\n 'Hive',\n 'how',\n 'how',\n 'if',\n 'if',\n 'if',\n 'if',\n 'in',\n 'in',\n 'in',\n 'in',\n 'in',\n 'including',\n 'including',\n 'including',\n 'individual',\n 'instance:',\n 'instructions.',\n 'Interactive',\n 'Interactive',\n 'is',\n 'is',\n 'is',\n 'is',\n 'is',\n 'is',\n 'It',\n 'It',\n 'its',\n 'Java,',\n 'latest',\n 'learning,',\n 'library',\n 'locally',\n 'locally',\n 'locally.',\n 'machine',\n 'Many',\n 'MASTER',\n 'MASTER=spark://host:7077',\n 'Maven](http://maven.apache.org/).',\n 'mesos://',\n 'MLlib',\n 'module,',\n 'More',\n 'must',\n 'N',\n 'name',\n 'need',\n 'no',\n 'not',\n 'Note',\n 'of',\n 'of',\n 'of',\n 'of',\n 'of',\n 'on',\n 'on',\n 'on',\n 'on',\n 'on',\n 'Once',\n 'one',\n 'one',\n 'Online',\n 'online',\n 'only',\n 'optimized',\n 'or',\n 'or',\n 'or',\n 'other',\n 'overview',\n 'package',\n 'package.',\n 'package.)',\n 'page](http://spark.apache.org/documentation.html)',\n 'params',\n 'particular',\n 'particular',\n 'Pi',\n 'Please',\n 'Please',\n 'Please',\n 'pre-built',\n 'prefer',\n 'print',\n 'processing,',\n 'processing.',\n 'programming',\n 'Programs',\n 'programs',\n 'programs',\n 'programs,',\n 'project',\n 'protocols',\n 'provides',\n 'Python',\n 'Python',\n 'Python,',\n 'Python,',\n 'R,',\n 'README',\n 'refer',\n 'refer',\n 'requires',\n 'return',\n 'return',\n 'rich',\n 'run',\n 'run',\n 'run',\n 'run',\n 'run',\n 'run',\n 'run',\n 'run:',\n 'running',\n 'Running',\n 'runs.',\n 'same',\n 'sample',\n 'sc.parallelize(1',\n 'sc.parallelize(range(1000)).count()',\n 'Scala',\n 'Scala',\n 'Scala,',\n 'scala>',\n 'see',\n 'set',\n 'set',\n 'setup',\n 'several',\n 'Shell',\n 'Shell',\n 'shell:',\n 'shell:',\n 'should',\n 'should',\n 'site,',\n 'Spark',\n 'Spark',\n 'Spark',\n 'Spark',\n 'Spark',\n 'Spark',\n 'Spark',\n 'Spark',\n 'Spark',\n 'Spark',\n 'Spark',\n 'Spark',\n 'Spark',\n 'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n 'Spark.',\n 'spark://',\n 'Spark](#building-spark).',\n 'SparkPi',\n 'SparkPi',\n 'SQL',\n 'SQL',\n 'start',\n 'storage',\n 'stream',\n 'Streaming',\n 'submit',\n 'supports',\n 'supports',\n 'system',\n 'systems.',\n 'talk',\n 'Testing',\n 'Tests',\n 'tests',\n 'tests',\n 'tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).',\n 'that',\n 'that',\n 'the',\n 'the',\n 'the',\n 'The',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'the',\n 'them,',\n 'This',\n 'this',\n 'This',\n 'thread,',\n 'threads.',\n 'Thriftserver',\n 'through',\n 'To',\n 'to',\n 'to',\n 'to',\n 'To',\n 'to',\n 'to',\n 'to',\n 'to',\n 'to',\n 'to',\n 'to',\n 'to',\n 'to',\n 'to',\n 'to',\n 'tools',\n 'Try',\n 'URL,',\n 'usage',\n 'use',\n 'use',\n 'use',\n 'uses',\n 'using',\n 'using',\n 'using:',\n 'variable',\n 'version',\n 'Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n 'Versions',\n 'versions',\n 'way',\n 'web',\n 'when',\n 'which',\n 'which',\n 'wiki](https://cwiki.apache.org/confluence/display/SPARK).',\n 'will',\n 'with',\n 'with',\n 'with',\n 'YARN,',\n 'You',\n 'you',\n 'you',\n 'you',\n 'You',\n 'You',\n 'you',\n 'your']"
     ]
    }
   ],
   "source": [
    "# 4. sort in alphabetical order\n",
    "words_rdd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "sorted_rdd = words_rdd.sortBy(lambda word: word.lower())\n",
    "sorted_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "103ffb91-f715-4b9d-b6ac-8c47fb699233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: [('', 67),\n ('the', 22),\n ('to', 16),\n ('spark', 13),\n ('for', 13),\n ('and', 11),\n ('a', 9),\n ('##', 8),\n ('run', 7),\n ('you', 7),\n ('is', 6),\n ('can', 6),\n ('in', 5),\n ('of', 5),\n ('on', 5),\n ('documentation', 4),\n ('also', 4),\n ('example', 4),\n ('if', 4),\n ('an', 3),\n ('this', 3),\n ('use', 3),\n ('programs', 3),\n ('tests', 3),\n ('hadoop', 3),\n ('including', 3),\n ('building', 3),\n ('build', 3),\n ('with', 3),\n ('or', 3),\n ('please', 3),\n ('supports', 2),\n ('set', 2),\n ('online', 2),\n ('[project', 2),\n ('using', 2),\n ('do', 2),\n ('at', 2),\n ('scala', 2),\n ('shell', 2),\n ('following', 2),\n ('1000:', 2),\n ('python', 2),\n ('./bin/run-example', 2),\n ('examples', 2),\n ('locally', 2),\n ('class', 2),\n ('guidance', 2),\n ('versions', 2),\n ('hadoop,', 2),\n ('refer', 2),\n ('particular', 2),\n ('hive', 2),\n ('general', 2),\n ('cluster', 2),\n ('it', 2),\n ('python,', 2),\n ('that', 2),\n ('sql', 2),\n ('detailed', 2),\n ('interactive', 2),\n ('shell:', 2),\n ('command,', 2),\n ('which', 2),\n ('should', 2),\n ('return', 2),\n ('`examples`', 2),\n ('one', 2),\n ('sparkpi', 2),\n ('running', 2),\n ('be', 2),\n ('how', 2),\n ('#', 1),\n ('data.', 1),\n ('provides', 1),\n ('high-level', 1),\n ('java,', 1),\n ('r,', 1),\n ('optimized', 1),\n ('engine', 1),\n ('computation', 1),\n ('analysis.', 1),\n ('tools', 1),\n ('dataframes,', 1),\n ('machine', 1),\n ('learning,', 1),\n ('graph', 1),\n ('processing,', 1),\n ('streaming', 1),\n ('latest', 1),\n ('programming', 1),\n ('guide,', 1),\n ('wiki](https://cwiki.apache.org/confluence/display/spark).', 1),\n ('readme', 1),\n ('only', 1),\n ('basic', 1),\n ('instructions.', 1),\n ('maven](http://maven.apache.org/).', 1),\n ('run:', 1),\n ('(you', 1),\n ('downloaded', 1),\n ('more', 1),\n ('project', 1),\n ('site,', 1),\n ('spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n ('way', 1),\n ('start', 1),\n ('try', 1),\n ('scala>', 1),\n ('1000).count()', 1),\n ('alternatively,', 1),\n ('several', 1),\n ('them,', 1),\n ('`./bin/run-example', 1),\n ('[params]`.', 1),\n ('example:', 1),\n ('master', 1),\n ('variable', 1),\n ('when', 1),\n ('spark://', 1),\n ('url,', 1),\n ('yarn,', 1),\n ('\"local\"', 1),\n ('\"local[n]\"', 1),\n ('abbreviated', 1),\n ('name', 1),\n ('package.', 1),\n ('instance:', 1),\n ('master=spark://host:7077', 1),\n ('print', 1),\n ('usage', 1),\n ('help', 1),\n ('no', 1),\n ('params', 1),\n ('are', 1),\n ('once', 1),\n ('built,', 1),\n ('using:', 1),\n ('./dev/run-tests', 1),\n ('module,', 1),\n ('individual', 1),\n ('tests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools).',\n  1),\n ('uses', 1),\n ('library', 1),\n ('hdfs', 1),\n ('other', 1),\n ('storage', 1),\n ('systems.', 1),\n ('have', 1),\n ('changed', 1),\n ('different', 1),\n ('must', 1),\n ('against', 1),\n ('version', 1),\n ('[\"specifying', 1),\n ('version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n  1),\n ('distribution', 1),\n ('distributions.', 1),\n ('configuration', 1),\n ('guide](http://spark.apache.org/docs/latest/configuration.html)', 1),\n ('overview', 1),\n ('configure', 1),\n ('spark.', 1),\n ('apache', 1),\n ('fast', 1),\n ('computing', 1),\n ('system', 1),\n ('big', 1),\n ('apis', 1),\n ('scala,', 1),\n ('graphs', 1),\n ('data', 1),\n ('rich', 1),\n ('higher-level', 1),\n ('mllib', 1),\n ('graphx', 1),\n ('stream', 1),\n ('processing.', 1),\n ('<http://spark.apache.org/>', 1),\n ('find', 1),\n ('documentation,', 1),\n ('web', 1),\n ('page](http://spark.apache.org/documentation.html)', 1),\n ('file', 1),\n ('contains', 1),\n ('setup', 1),\n ('built', 1),\n ('[apache', 1),\n ('its', 1),\n ('programs,', 1),\n ('build/mvn', 1),\n ('-dskiptests', 1),\n ('clean', 1),\n ('package', 1),\n ('not', 1),\n ('need', 1),\n ('pre-built', 1),\n ('package.)', 1),\n ('available', 1),\n ('from', 1),\n ('[\"building', 1),\n ('easiest', 1),\n ('through', 1),\n ('./bin/spark-shell', 1),\n ('sc.parallelize(1', 1),\n ('prefer', 1),\n ('./bin/pyspark', 1),\n ('>>>', 1),\n ('sc.parallelize(range(1000)).count()', 1),\n ('comes', 1),\n ('sample', 1),\n ('directory.', 1),\n ('<class>', 1),\n ('will', 1),\n ('pi', 1),\n ('locally.', 1),\n ('environment', 1),\n ('submit', 1),\n ('cluster.', 1),\n ('mesos://', 1),\n ('\"yarn\"', 1),\n ('thread,', 1),\n ('n', 1),\n ('threads.', 1),\n ('many', 1),\n ('given.', 1),\n ('testing', 1),\n ('first', 1),\n ('requires', 1),\n ('[building', 1),\n ('spark](#building-spark).', 1),\n ('see', 1),\n ('[run', 1),\n ('note', 1),\n ('about', 1),\n ('core', 1),\n ('talk', 1),\n ('hadoop-supported', 1),\n ('because', 1),\n ('protocols', 1),\n ('same', 1),\n ('your', 1),\n ('runs.', 1),\n ('thriftserver', 1),\n ('[configuration', 1)]"
     ]
    }
   ],
   "source": [
    "# 5. sort from most to least frequent word\n",
    "# Step 1: Split the text into words and count occurrences\n",
    "word_counts = rdd.flatMap(lambda x: x.split(\" \")) \\\n",
    "                 .map(lambda word: (word.lower(), 1)) \\\n",
    "                 .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Step 2: Sort the word counts from most frequent to least frequent\n",
    "sorted_word_counts = word_counts.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Step 3: Collect and display the sorted results\n",
    "sorted_word_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4eaa9bb-2882-41fc-b676-2c24844e80bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: [' Apache Spark',\n '',\n 'Spark is a fast and general cluster computing system for Big Data It provides',\n 'highlevel APIs in Scala Java Python and R and an optimized engine that',\n 'supports general computation graphs for data analysis It also supports a',\n 'rich set of higherlevel tools including Spark SQL for SQL and DataFrames',\n 'MLlib for machine learning GraphX for graph processing',\n 'and Spark Streaming for stream processing',\n '',\n 'httpsparkapacheorg',\n '',\n '',\n ' Online Documentation',\n '',\n 'You can find the latest Spark documentation including a programming',\n 'guide on the project web pagehttpsparkapacheorgdocumentationhtml',\n 'and project wikihttpscwikiapacheorgconfluencedisplaySPARK',\n 'This README file only contains basic setup instructions',\n '',\n ' Building Spark']"
     ]
    }
   ],
   "source": [
    "# 6.** remove punctuations \n",
    "import re\n",
    "cleaned_rdd = rdd.map(lambda line: re.sub(r'[^\\w\\s]', '', line))\n",
    "cleaned_rdd.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e683e60a-c072-4ace-8ceb-0f615bb9bbcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. What does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "764898e3-0cb4-41c2-8a9c-4d3a712d43bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brooke: 22.50\nDenny: 31.00\nTD: 35.00\nJules: 30.00\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD of tuples (name, age)\n",
    "dataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    "(\"TD\", 35), (\"Brooke\", 25)])\n",
    "\n",
    "# Try to undestand what this code does (line by line)\n",
    "agesRDD = (dataRDD\n",
    "  .map(lambda x: (x[0], (x[1], 1)))\n",
    "  .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "  .map(lambda x: (x[0], x[1][0]/x[1][1])))\n",
    "\n",
    "# Collect and print results\n",
    "for name, avg_age in agesRDD.collect():\n",
    "    print(f\"{name}: {avg_age:.2f}\")\n",
    "\n",
    "#This Spark code computes the average age for each unique name in the dataRDD. Here's a step-by-step breakdown:"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Lab1_RDD_practice",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}